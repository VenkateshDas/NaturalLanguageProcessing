{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__load', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_unload', 'subdir', 'unicode_repr']\n"
     ]
    }
   ],
   "source": [
    "print(dir(gutenberg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11793318\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    text += gutenberg.raw(file_id)\n",
    " \n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ami', 'ophe', 'cassi', 'malc', 'clit', 'mur', 'b', 'banq', 'ed', 'moo', 'calp', 'brut', 'cic', 'phil', 'ros', 'a.s', \"'w\", 'ara', 'oct', 'stra', 'esq', 'var', 'dut', 'clo', 'cop', 'mal', 'macd', 'ment', 'pind', 'p.h', 'foh', 'jun', 'octa', 'w', \"'dr\", 'hora', 'talb', 'laer', 'finsb', 'p.s', 'u', 'cai', 'k.c', 'pol', 'ro', 'l10', 'feb', 'por', 'dag', 'lep', 'cly', 's.w.f', 'ang', 'dard', 'polon', 'deci', 'n', 'br', 'rosin', 'guil', 'macb', 'g', 'wm', 'a.d', 'ger', 'prima', 'rev', 'trans', 'sw', 'u.s', 'mac', 'fla', 'cob', 'f', 'c', 'i.e', 'fred', 'boa', 'doct', 'gho', 'burs', 'amb', 'vol', 'v.e', 'osr', 'poe', 'y.sey', 'cin', 'g.k', 'fran', 'etc', 'ave', 'ult', 'n.e', 'qu', 'lbs', 'syw', 'mss', 'p.m', '1.ple', \"'mr\", 'bap', 'j', 'hon', 'ibid', 'dec', 'gent', 'gen', 'len', 'xxx', 'ple', 'ant', 'vat', 'o.r', 'messa', 'hec', 'cyn', 'cath', 'cass', 'k', 'tit', 'm.d', 'ser', 'sey', 'bru', 'luc', 'hag', 'mes', 'p', 'treb', 'm.p', 'xx', 'ely', 'm', 'mt', 'cas'}\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'break_decision': False,\n",
      " 'collocation': True,\n",
      " 'period_index': 2,\n",
      " 'reason': 'known collocation (both words)',\n",
      " 'text': 'Mr. James',\n",
      " 'type1': 'mr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'james',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-UC', 'BEG-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 20,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Dr. Brown',\n",
      " 'type1': 'dr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'brown',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-UC', 'MID-LC', 'BEG-UC', 'MID-UC', 'UNK-LC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 50,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'today. I',\n",
      " 'type1': 'today.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'i',\n",
      " 'type2_is_sent_starter': True,\n",
      " 'type2_ortho_contexts': {'BEG-LC',\n",
      "                          'BEG-UC',\n",
      "                          'MID-LC',\n",
      "                          'MID-UC',\n",
      "                          'UNK-LC',\n",
      "                          'UNK-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print ('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._params.abbrev_types.add('dr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. James told me Dr. Brown is not available today.', 'I will try tomorrow.']\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'break_decision': False,\n",
      " 'collocation': True,\n",
      " 'period_index': 2,\n",
      " 'reason': 'known collocation (both words)',\n",
      " 'text': 'Mr. James',\n",
      " 'type1': 'mr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'james',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-UC', 'BEG-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': None,\n",
      " 'collocation': False,\n",
      " 'period_index': 20,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Dr. Brown',\n",
      " 'type1': 'dr.',\n",
      " 'type1_in_abbrs': True,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'brown',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-UC', 'MID-LC', 'BEG-UC', 'MID-UC', 'UNK-LC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 50,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'today. I',\n",
      " 'type1': 'today.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'i',\n",
      " 'type2_is_sent_starter': True,\n",
      " 'type2_ortho_contexts': {'BEG-LC',\n",
      "                          'BEG-UC',\n",
      "                          'MID-LC',\n",
      "                          'MID-UC',\n",
      "                          'UNK-LC',\n",
      "                          'UNK-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print ('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraph = \"\"\"\n",
    "“When I was 16 I visited Anne Frank’s house with Miep Gies, the woman who bravely hid Anne and her family when the Nazis were rounding up Jews in Amsterdam and much of Europe,” Portman captioned a snapshot of herself as a teen, standing in front of the trick-staircase that opened the door to the annex. “Today, I shudder at the thought of a young girl hiding somewhere in my own country, afraid to turn on her light or make a noise or play outside lest she get rounded up by our government.” She added the hashtags, “notinmyname” and “notinmycountry.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n“When I was 16 I visited Anne Frank’s house with Miep Gies, the woman who bravely hid Anne and her family when the Nazis were rounding up Jews in Amsterdam and much of Europe,” Portman captioned a snapshot of herself as a teen, standing in front of the trick-staircase that opened the door to the annex.', '“Today, I shudder at the thought of a young girl hiding somewhere in my own country, afraid to turn on her light or make a noise or play outside lest she get rounded up by our government.” She added the hashtags, “notinmyname” and “notinmycountry.”']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(test_paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 303,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'annex. “Today,',\n",
      " 'type1': 'annex.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': '“today',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': set(),\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for decision in tokenizer.debug_decisions(test_paragraph):\n",
    "    pprint(decision)\n",
    "    print ('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
