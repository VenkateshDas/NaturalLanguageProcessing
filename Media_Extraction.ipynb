{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Media_Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKRIgk14GF3SrbTTQr3/NV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkateshDas/NaturalLanguageProcessing/blob/master/Media_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ1KYbXy1McW"
      },
      "source": [
        "# URL's to scrape\n",
        "\n",
        "1. https://www.zeit.de/gesellschaft/zeitgeschehen/2019-12/einbuergerungsgesetz-indien-proteste-demonstrationen-tote\n",
        "2. https://www.zeit.de/campus/2019-12/studentenproteste-indien-einwanderungsgesetz-staatsbuergerschaftsreform\n",
        "3. https://www.zeit.de/politik/ausland/2019-12/indien-neu-delhi-proteste-staatsbuergerschaftsreform\n",
        "4. https://www.faz.net/aktuell/wirtschaft/wachstumskrise-ganz-indien-ist-in-protestlaune-16575265.html\n",
        "5. https://www.faz.net/aktuell/wirtschaft/millionen-inder-streiken-gegen-wirtschaftspolitik-16571450.html\n",
        "6. https://www.fr.de/politik/demokratie-gefahr-13459676.html\n",
        "7. https://www.spiegel.de/politik/ausland/muslime-ausgeschlossen-grossdemonstrationen-gegen-indisches-staatsbuergerschaftsrecht-a-1303600.html\n",
        "8. https://www.spiegel.de/politik/indien-premier-narendra-modi-ein-nationalist-und-seine-200-millionen-feinde-a-00000000-0002-0001-0000-000168763995\n",
        "9. https://www.spiegel.de/politik/ausland/indien-regierung-entzieht-jammu-und-kaschmir-den-sonderstatus-a-1280541.html\n",
        "10. https://www.spiegel.de/politik/ausland/indien-hebt-sonderstatus-fuer-kaschmir-in-der-verfassung-auf-a-1280455.html\n",
        "11. https://www.spiegel.de/international/world/deathly-silence-an-inside-look-at-kashmir-a-1296450.html\n",
        "12. https://www.dw.com/en/india-citizenship-law-protests-spearheaded-by-women/a-52108903\n",
        "13. https://www.dw.com/en/fresh-student-protests-hit-india-over-university-attack/a-51897307\n",
        "14. https://www.dw.com/en/indias-new-citizenship-law-ignites-religious-tensions/g-51809014\n",
        "15. https://www.dw.com/en/indian-state-shuts-down-internet-ahead-of-protests/a-51807095\n",
        "16. https://www.dw.com/en/india-curfew-internet-closures-as-deadly-citizenship-protests-continue/a-51747818\n",
        "16. https://www.dw.com/en/india-stripped-kashmir-of-autonomy-to-end-separatism-modi-says/a-49948685\n",
        "17. https://www.dw.com/en/kashmir-lockdown-an-uneasy-calm-prevails/a-49965552\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1a63XbL06Dk"
      },
      "source": [
        "# not accessible : 'https://www.faz.net/aktuell/wirtschaft/wachstumskrise-ganz-indien-ist-in-protestlaune-16575265.html' - 2 paragraphs\n",
        "# 'https://www.zeit.de/campus/2019-12/studentenproteste-indien-einwanderungsgesetz-staatsbuergerschaftsreform'\n",
        "\n",
        "zeit_url = ['https://www.zeit.de/gesellschaft/zeitgeschehen/2019-12/einbuergerungsgesetz-indien-proteste-demonstrationen-tote',\n",
        "            'https://www.zeit.de/politik/ausland/2019-12/indien-neu-delhi-proteste-staatsbuergerschaftsreform']\n",
        "faz_url = ['https://www.faz.net/aktuell/wirtschaft/wachstumskrise-ganz-indien-ist-in-protestlaune-16575265.html',\n",
        "           'https://www.faz.net/aktuell/wirtschaft/millionen-inder-streiken-gegen-wirtschaftspolitik-16571450.html']\n",
        "fr_url = ['https://www.fr.de/politik/demokratie-gefahr-13459676.html']\n",
        "spiegel_url = ['https://www.spiegel.de/politik/ausland/muslime-ausgeschlossen-grossdemonstrationen-gegen-indisches-staatsbuergerschaftsrecht-a-1303600.html',\n",
        "               'https://www.spiegel.de/politik/ausland/indien-regierung-entzieht-jammu-und-kaschmir-den-sonderstatus-a-1280541.html',\n",
        "               'https://www.spiegel.de/politik/ausland/indien-hebt-sonderstatus-fuer-kaschmir-in-der-verfassung-auf-a-1280455.html',\n",
        "               'https://www.spiegel.de/international/world/deathly-silence-an-inside-look-at-kashmir-a-1296450.html']\n",
        "dw_url = ['https://www.dw.com/en/india-citizenship-law-protests-spearheaded-by-women/a-52108903',\n",
        "          'https://www.dw.com/en/fresh-student-protests-hit-india-over-university-attack/a-51897307',\n",
        "          'https://www.dw.com/en/indias-new-citizenship-law-ignites-religious-tensions/g-51809014',\n",
        "          'https://www.dw.com/en/indian-state-shuts-down-internet-ahead-of-protests/a-51807095',\n",
        "          'https://www.dw.com/en/india-curfew-internet-closures-as-deadly-citizenship-protests-continue/a-51747818',\n",
        "          'https://www.dw.com/en/india-stripped-kashmir-of-autonomy-to-end-separatism-modi-says/a-49948685',\n",
        "          'https://www.dw.com/en/kashmir-lockdown-an-uneasy-calm-prevails/a-49965552']\n",
        "\n",
        "url_lists = {'zeit':zeit_url,'faz':faz_url,'fr':fr_url,'spiegel':spiegel_url,'dw':dw_url}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2WQsvr79Wi6"
      },
      "source": [
        "## install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtxI7lvx9VHw",
        "outputId": "986aa2b6-4201-4cfc-f029-4a60c18f2162"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15YP7w_4txcY",
        "outputId": "29b58419-a459-4380-de69-3e03d61dc686"
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.227.219.19)] [6 \r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,309 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [309 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.6 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,146 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,376 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,881 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [284 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,714 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [877 kB]\n",
            "Get:23 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [43.6 kB]\n",
            "Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.2 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [66.5 kB]\n",
            "Fetched 11.4 MB in 4s (3,246 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 61 not upgraded.\n",
            "Need to get 81.0 MB of archives.\n",
            "After this operation, 273 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
            "Fetched 81.0 MB in 2s (52.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUUDGkOP43iw",
        "outputId": "7dc2bc82-3d2f-435b-d019-35881c019d1e"
      },
      "source": [
        "!pip install spiegel-scraper"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spiegel-scraper\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/bf/08312e038cdc6b699123acebedee3ec0f307653578fc28abbdea25f69fc4/spiegel_scraper-1.1.1-py3-none-any.whl\n",
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/c4/b5ddc3eeac974d85055d88c1e6b62cc492fc1a93dbe3b66a45a756a7b807/dateparser-1.0.0-py2.py3-none-any.whl (279kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 11.2MB/s \n",
            "\u001b[?25hCollecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from spiegel-scraper) (4.2.6)\n",
            "Collecting tldextract\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from spiegel-scraper) (2.23.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser->spiegel-scraper) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser->spiegel-scraper) (2.8.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser->spiegel-scraper) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser->spiegel-scraper) (2018.9)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.6/dist-packages (from tldextract->spiegel-scraper) (3.0.12)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract->spiegel-scraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->spiegel-scraper) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->spiegel-scraper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->spiegel-scraper) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser->spiegel-scraper) (1.15.0)\n",
            "Installing collected packages: dateparser, cssselect, requests-file, tldextract, spiegel-scraper\n",
            "Successfully installed cssselect-1.1.0 dateparser-1.0.0 requests-file-1.5.1 spiegel-scraper-1.1.1 tldextract-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "fa3aBQ7Z4Zrj",
        "outputId": "71351b86-e5bb-4cfd-c763-aa14d664d538"
      },
      "source": [
        "# Import necessary libraries\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from datetime import date\n",
        "import spiegel_scraper as spon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from germansentiment import SentimentModel\n",
        "import re\n",
        "\n",
        "# configuration\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "german_stop_words = stopwords.words('german')\n",
        "english_stop_word = stopwords.words('english')\n",
        "stopwords = german_stop_words + english_stop_word"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aed1cf9f2806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgermansentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'germansentiment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNcSLRhk3MZc"
      },
      "source": [
        "def extract_zeit(res):\n",
        "\n",
        "    zeit_dict = {}\n",
        "    para_list = []\n",
        "    \n",
        "    title_tags = res.findAll(\"span\", {\"class\": \"article-heading__title\"})\n",
        "    summary_tags = res.findAll('div',{'class': 'summary'})\n",
        "    times = res.findAll('time',{'class':'metadata__date'})\n",
        "    para_tags = res.findAll('p',{'class':'paragraph article__item'})\n",
        "    for title_tag in title_tags:\n",
        "        title = title_tag.getText()\n",
        "        zeit_dict['title'] = title\n",
        "    for summary_tag in summary_tags:\n",
        "        summary = summary_tag.getText()\n",
        "        zeit_dict['summary'] = summary\n",
        "    for para_tag in para_tags:\n",
        "        para_list.append(para_tag.getText())\n",
        "    for time in times:\n",
        "        t = time.getText()\n",
        "        zeit_dict['time'] = t\n",
        "    \n",
        "    content = \" \".join(para_list)\n",
        "    zeit_dict['content'] = content\n",
        "    \n",
        "    return zeit_dict\n",
        "    \n",
        "def extract_faz(res):\n",
        "\n",
        "    try: \n",
        "        faz_dict = {}\n",
        "        para_list = []\n",
        "\n",
        "        title_tags = res.findAll(\"span\", {\"class\": \"atc-HeadlineText\"})\n",
        "        summary_tags = res.findAll('p',{'class': 'atc-IntroText'})\n",
        "        times = res.findAll('time',{'class':'atc-MetaTime'})\n",
        "        first_para_tags = res.findAll('p',{'class':'First atc-TextParagraph'})\n",
        "        para_tags = res.findAll('p',{'class':'atc-TextParagraph'})\n",
        "        for title_tag in title_tags:\n",
        "            title = title_tag.getText()\n",
        "            faz_dict['title'] = title\n",
        "        for summary_tag in summary_tags:\n",
        "            summary = summary_tag.getText()\n",
        "            faz_dict['summary'] = summary\n",
        "        for first_para in first_para_tags:\n",
        "            f_content = first_para.getText()\n",
        "        for para_tag in para_tags:\n",
        "            para_list.append(para_tag.getText())\n",
        "        for time in times:\n",
        "            t = time.getText()\n",
        "            faz_dict['time'] = t\n",
        "        \n",
        "        content = \" \".join(para_list)\n",
        "        faz_dict['content'] = f_content+content\n",
        "        \n",
        "    except: \n",
        "        print(\"Error\")\n",
        "\n",
        "    return faz_dict\n",
        "\n",
        "\n",
        "def extract_fr(url,res):\n",
        "\n",
        "    try : \n",
        "        fr_dict = {}\n",
        "        title_tags = res.findAll(\"h2\", {\"class\": \"id-Article-headline \"})\n",
        "        summary_tags = res.findAll('p',{'class': 'id-Article-content-item id-Article-content-item-summary'})\n",
        "        times = res.findAll('span',{'class':'id-DateTime-date'})\n",
        "\n",
        "        for title_tag in title_tags:\n",
        "            title = title_tag.getText()\n",
        "            fr_dict['title'] = title\n",
        "        for summary_tag in summary_tags:\n",
        "            summary = summary_tag.getText()\n",
        "            fr_dict['summary'] = summary\n",
        "        for time in times:\n",
        "            t = time.getText()\n",
        "            fr_dict['time'] = t\n",
        "            \n",
        "        \n",
        "        browser.get(url)\n",
        "        para_list = []\n",
        "        i = 2\n",
        "        while i <=15:\n",
        "            p = browser.find_element_by_xpath('//*[@id=\"id-js-Story\"]/div[2]/p['+str(i)+']')\n",
        "            para_list.append(p.text)\n",
        "            i = i + 1\n",
        "        content = \" \".join(para_list)\n",
        "        fr_dict['content'] = content\n",
        "\n",
        "    except Error as e: \n",
        "        print(e)\n",
        "    \n",
        "    return fr_dict\n",
        "    \n",
        "\n",
        "def extract_spiegel(url):\n",
        "\n",
        "    try : \n",
        "        spiegel_dict = {}\n",
        "        article_html = spon.article.html_by_url(url)\n",
        "        article_from_html = spon.article.scrape_html(article_html)\n",
        "        title = article_from_html['headline']['main']\n",
        "        time = article_from_html['date_published']\n",
        "        summary = article_from_html['intro']\n",
        "        content = article_from_html['text']\n",
        "\n",
        "        spiegel_dict = {'time':time,'title':title,'summary':summary,'content':content}\n",
        "\n",
        "    except:\n",
        "        print(\"Error\")\n",
        "\n",
        "    return spiegel_dict\n",
        "\n",
        "def extract_dw(url,res):\n",
        "\n",
        "    try : \n",
        "    \n",
        "        dw_dict = {}\n",
        "\n",
        "        browser.get(url)\n",
        "        t = browser.find_element_by_xpath('//*[@id=\"bodyContent\"]/div[2]/div/ul/li[1]')\n",
        "        time = t.text\n",
        "        dw_dict['time'] = time\n",
        "        \n",
        "        title_tags = res.findAll(\"h1\")\n",
        "        summary_tags = res.findAll('p',{'class': 'intro'})\n",
        "        for title_tag in title_tags:\n",
        "            title = title_tag.getText()\n",
        "            dw_dict['title'] = title\n",
        "        for summary_tag in summary_tags:\n",
        "            summary = summary_tag.getText()\n",
        "            dw_dict['summary'] = summary\n",
        "        \n",
        "        para_list = []\n",
        "        i = 1\n",
        "\n",
        "        while i:\n",
        "\n",
        "            try:\n",
        "                p = browser.find_element_by_xpath('//*[@id=\"bodyContent\"]/div[1]/div[3]/div/p['+str(i)+']')\n",
        "                para = p.text\n",
        "                para_list.append(p.text)\n",
        "                i = i + 1\n",
        "            except:\n",
        "                break\n",
        "        content = \" \".join(para_list)\n",
        "        dw_dict['content'] = content\n",
        "\n",
        "    except :\n",
        "        print(\"Error\")\n",
        "\n",
        "    return dw_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXQSvdHGAkth"
      },
      "source": [
        "url_lists = {'zeit':zeit_url,'faz':faz_url,'fr':fr_url,'spiegel':spiegel_url,'dw':dw_url}\n",
        "columns = ['time','title','summary','content']\n",
        "\n",
        "media_list = []\n",
        "times_list = []\n",
        "titles_list = []\n",
        "summary_list = []\n",
        "content_list = []\n",
        "\n",
        "\n",
        "for media,urls in url_lists.items():\n",
        "\n",
        "    print(media)\n",
        "\n",
        "    for url in urls:\n",
        "\n",
        "        try:\n",
        "\n",
        "            html = urlopen(url)\n",
        "\n",
        "        except HTTPError as e:\n",
        "\n",
        "            print(e)\n",
        "\n",
        "        except URLError:\n",
        "\n",
        "            print(\"Server down or incorrect domain\")\n",
        "\n",
        "        else:\n",
        "\n",
        "            res = BeautifulSoup(html.read(),\"html5lib\")\n",
        "\n",
        "            if media == 'zeit':\n",
        "                zeit_dict = extract_zeit(res)\n",
        "                times_list.append(zeit_dict['time'])\n",
        "                media_list.append(media)\n",
        "                titles_list.append(zeit_dict['title'])\n",
        "                summary_list.append(zeit_dict['summary'])\n",
        "                content_list.append(zeit_dict['content'])\n",
        "            elif media == 'faz':\n",
        "                faz_dict = extract_faz(res)\n",
        "                times_list.append(faz_dict['time'])\n",
        "                media_list.append(media)\n",
        "                titles_list.append(faz_dict['title'])\n",
        "                summary_list.append(faz_dict['summary'])\n",
        "                content_list.append(faz_dict['content'])\n",
        "                \n",
        "            elif media == 'fr':\n",
        "                fr_dict = extract_fr(url,res)\n",
        "                times_list.append(fr_dict['time'])\n",
        "                media_list.append(media)\n",
        "                titles_list.append(fr_dict['title'])\n",
        "                summary_list.append(fr_dict['summary'])\n",
        "                content_list.append(fr_dict['content'])\n",
        "                \n",
        "            elif media == 'spiegel':\n",
        "                spiegel_dict = extract_spiegel(url)\n",
        "                times_list.append(spiegel_dict['time'])\n",
        "                media_list.append(media)\n",
        "                titles_list.append(spiegel_dict['title'])\n",
        "                summary_list.append(spiegel_dict['summary'])\n",
        "                content_list.append(spiegel_dict['content'])\n",
        "                \n",
        "            elif media== 'dw':\n",
        "                dw_dict = extract_dw(url,res)\n",
        "                times_list.append(dw_dict['time'])\n",
        "                media_list.append(media)\n",
        "                titles_list.append(dw_dict['title'])\n",
        "                summary_list.append(dw_dict['summary'])\n",
        "                content_list.append(dw_dict['content'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C54ExmxLH82B"
      },
      "source": [
        "articles_dataframe = pd.DataFrame({'time':times_list,'media':media_list,'title':titles_list,'summary':summary_list,'content':content_list})\n",
        "articles_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VloaGk3Qbof"
      },
      "source": [
        "articles_dataframe = articles_dataframe.drop(labels='time',axis=1)\n",
        "articles_dataframe['content'].iloc[11] = 'No content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BLQtFk0hDZq"
      },
      "source": [
        "articles_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1QRKXuVLnKc"
      },
      "source": [
        "def clean_txt(input_txt):\n",
        "\n",
        "    #removing hashtags,emojis,stopwords\n",
        "    input_txt=re.sub(r'#[\\w]*','',input_txt)\n",
        "    input_txt=input_txt.encode(\"ascii\",\"ignore\")\n",
        "    input_txt=input_txt.decode()\n",
        "\n",
        "    #removing punctuation,numbers,whitespace,tabs  \n",
        "    res=re.sub(r'[^\\w\\s]', '', input_txt.lower())\n",
        "    res=re.sub('\\s+',' ',res)\n",
        "    res=re.sub('\\t+','',res)\n",
        "    ##removing links\n",
        "    res=re.sub(r'https[\\w]*', '', res, flags=re.MULTILINE)\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPYQeuUgOp9a"
      },
      "source": [
        "articles_dataframe['clean_title'] = np.vectorize(clean_txt)(articles_dataframe['title'])\n",
        "articles_dataframe['clean_summary'] = np.vectorize(clean_txt)(articles_dataframe['summary'])\n",
        "articles_dataframe['clean_content'] = np.vectorize(clean_txt)(articles_dataframe['content'])\n",
        "articles_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rAGCKGRSVLE"
      },
      "source": [
        "sns.set()\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "g=sns.countplot(y=\"media\",  data=articles_dataframe,order=articles_dataframe['media'].value_counts().iloc[:5].index)\n",
        "plt.title(\"Article Source\")\n",
        "plt.ylabel(\"Newspaper Media\")\n",
        "plt.xlabel(\"Number of articles\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-vBhlfRUh2r"
      },
      "source": [
        "def stop_word_removal(x):\n",
        "    res=re.sub(r'[^\\w\\s]', '', x.lower())\n",
        "    text=re.sub('\\s+',' ',res)\n",
        "    text=re.sub('\\t+','',text)\n",
        "    token = wordpunct_tokenize(text)\n",
        "    return [w for w in token if not w in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nNi1hMJfr79"
      },
      "source": [
        "articles_dataframe['cleaned_tokenised_content'] =articles_dataframe['content'].apply(stop_word_removal)\n",
        "articles_dataframe['cleaned_tokenised_summary'] =articles_dataframe['summary'].apply(stop_word_removal)\n",
        "articles_dataframe['cleaned_tokenised_title'] =articles_dataframe['title'].apply(stop_word_removal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhvExvDeMz20"
      },
      "source": [
        "articles_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4r9Qwvzpe1e"
      },
      "source": [
        "def join_list(x):\n",
        "    return ' '.join(x)\n",
        "\n",
        "articles_dataframe['joined_content'] =articles_dataframe['cleaned_tokenised_content'].apply(join_list)\n",
        "articles_dataframe['joined_summary'] =articles_dataframe['cleaned_tokenised_summary'].apply(join_list)\n",
        "articles_dataframe['joined_title'] =articles_dataframe['cleaned_tokenised_title'].apply(join_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFNxEhm2jupJ"
      },
      "source": [
        "articles_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK8stsV2jMrD"
      },
      "source": [
        "def create_ngrams(sentences_token, n):\n",
        "    ngrams_list = []\n",
        "    for sentence in sentences_token:\n",
        "        if type(sentence) != list:\n",
        "            grams = list(ngrams(ast.literal_eval(sentence),n))\n",
        "        else :\n",
        "            grams = list(ngrams(sentence,n))\n",
        "        ngrams_list.append(grams)\n",
        "    return ngrams_list\n",
        "\n",
        "\n",
        "def ngram_counter(ngram_list,n):\n",
        "\n",
        "    total_ngrams_list = []\n",
        "    for ngram in ngram_list:\n",
        "        for gram in ngram:\n",
        "            total_ngrams_list.append(gram)\n",
        "    ngram_counter = Counter(total_ngrams_list)\n",
        "    most_used_ngram_counter = ngram_counter.most_common(n)\n",
        "\n",
        "    return total_ngrams_list,ngram_counter , most_used_ngram_counter\n",
        "\n",
        "\n",
        "def combine_ngrams(ngram_list):    \n",
        "    total_combined_ngram_list = []\n",
        "    for ngrams in ngram_list:\n",
        "        combined_ngram_list = []\n",
        "        for ngram in ngrams :\n",
        "            combined_ngram_list.append('_'.join(str(v) for v in ngram))\n",
        "        total_combined_ngram_list.append(combined_ngram_list)\n",
        "    return total_combined_ngram_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fKvKKrvkIlL"
      },
      "source": [
        "content = articles_dataframe['cleaned_tokenised_content']\n",
        "summary = articles_dataframe['cleaned_tokenised_summary']\n",
        "content_bigram_list = create_ngrams(content, 2)\n",
        "content_trigram_list = create_ngrams(content, 3)\n",
        "summary_bigram_list = create_ngrams(summary, 2)\n",
        "summary_trigram_list = create_ngrams(summary, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dr1YYNOkNoT"
      },
      "source": [
        "total_content_bigrams_list , bigram_counter , most_used_bigram_counter = ngram_counter(content_bigram_list,50)\n",
        "total_content_trigrams_list , trigram_counter , most_used_trigram_counter = ngram_counter(content_trigram_list,50)\n",
        "total_summary_bigrams_list , query_bigram_counter , query_most_used_bigram_counter = ngram_counter(summary_bigram_list,50)\n",
        "total_summary_trigrams_list , query_trigram_counter , query_most_used_trigram_counter = ngram_counter(summary_trigram_list,50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kjmw_g6l755"
      },
      "source": [
        "most_used_bigram = bigram_counter.most_common(30)\n",
        "most_used_bigram = dict(most_used_bigram)\n",
        "bigrams_series = pd.Series(most_used_bigram)\n",
        "fig_bigram = bigrams_series.sort_values().plot.barh(color='lightcoral', width=.9, figsize=(12, 8),title='Most used Bigrams in German Media about India')\n",
        "fig_bigram.figure.savefig('./media_bigram.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHgyDJ-RmtJW"
      },
      "source": [
        "most_used_trigram = trigram_counter.most_common(30)\n",
        "most_used_trigram = dict(most_used_trigram)\n",
        "trigrams_series = pd.Series(most_used_trigram)\n",
        "fig_trigram = trigrams_series.sort_values().plot.barh(color='tomato', width=.9, figsize=(12, 8),title='Most used Trigrams in German Media about India')\n",
        "fig_trigram.figure.savefig('./media_trigram.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La5Y_W7Cm5ed"
      },
      "source": [
        "contentcloud=' '.join([i for i in articles_dataframe['joined_content']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=22, max_font_size=110).generate(contentcloud)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('Content Wordcloud')\n",
        "plt.axis('off')\n",
        "plt.savefig(\"./contentcloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-f3lC3upPno"
      },
      "source": [
        "summarycloud=' '.join([i for i in articles_dataframe['joined_summary']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=22, max_font_size=110).generate(summarycloud)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('Summary Wordcloud')\n",
        "plt.axis('off')\n",
        "plt.savefig(\"./summarycloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8KV1RBq30u"
      },
      "source": [
        "titlecloud=' '.join([i for i in new_dataframe['joined_title']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=22, max_font_size=110).generate(summarycloud)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('Title Wordcloud')\n",
        "plt.axis('off')\n",
        "plt.savefig(\"./titlecloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xP6PZkyrb-X"
      },
      "source": [
        "#Sentiment Analysis \n",
        "\n",
        "!pip install germansentiment\n",
        "!pip install vader-multi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd8yAiLsnNZs"
      },
      "source": [
        "len(articles_dataframe['cleaned_tokenised_content'].iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGpNauLk0VWD"
      },
      "source": [
        "def sentiment_transformer(text):\n",
        "\n",
        "    model = SentimentModel()\n",
        "\n",
        "    result = model.predict_sentiment(text)\n",
        "    print(result)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn0AlmCVqGZ8"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "text = articles_dataframe['summary'].iloc[8]\n",
        "analyzer.polarity_scores(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLnrbLz7onhe"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = articles_dataframe['summary'].iloc[1]\n",
        "sentences = sent_tokenize(text)\n",
        "sentiment_transformer(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3BBj0GF7a2Q"
      },
      "source": [
        "articles_dataframe.to_csv('german_media_sentiment.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm421OBGSknV"
      },
      "source": [
        "media_group = articles_dataframe.groupby('media')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFSZOid2txlo"
      },
      "source": [
        "dw_df = media_group.get_group('dw')\n",
        "zeit_df = media_group.get_group('zeit')\n",
        "faz_df = media_group.get_group('faz')\n",
        "fr_df = media_group.get_group('fr')\n",
        "spiegel_df = media_group.get_group('spiegel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM1hbdrvuO_h"
      },
      "source": [
        " def grams_plot(df, name):   \n",
        "    content = df['cleaned_tokenised_content']\n",
        "    summary = df['cleaned_tokenised_summary']\n",
        "    content_bigram_list = create_ngrams(content, 2)\n",
        "    content_trigram_list = create_ngrams(content, 3)\n",
        "    summary_bigram_list = create_ngrams(summary, 2)\n",
        "    summary_trigram_list = create_ngrams(summary, 3)\n",
        "\n",
        "    total_content_bigrams_list , bigram_counter , most_used_bigram_counter = ngram_counter(content_bigram_list,50)\n",
        "    total_content_trigrams_list , trigram_counter , most_used_trigram_counter = ngram_counter(content_trigram_list,50)\n",
        "    total_summary_bigrams_list , query_bigram_counter , query_most_used_bigram_counter = ngram_counter(summary_bigram_list,50)\n",
        "    total_summary_trigrams_list , query_trigram_counter , query_most_used_trigram_counter = ngram_counter(summary_trigram_list,50)\n",
        "\n",
        "\n",
        "    most_used_bigram = bigram_counter.most_common(30)\n",
        "    most_used_bigram = dict(most_used_bigram)\n",
        "    bigrams_series = pd.Series(most_used_bigram)\n",
        "    fig_bigram = bigrams_series.sort_values().plot.barh(color='lightcoral', width=.9, figsize=(12, 8),title='Most used Bigrams in '+name+'about India')\n",
        "    fig_bigram.figure.savefig('./'+name+'_bigram.png')\n",
        "\n",
        "    most_used_trigram = trigram_counter.most_common(30)\n",
        "    most_used_trigram = dict(most_used_trigram)\n",
        "    trigrams_series = pd.Series(most_used_trigram)\n",
        "    fig_trigram = trigrams_series.sort_values().plot.barh(color='tomato', width=.9, figsize=(12, 8),title='Most used Trigrams in '+name+'about India')\n",
        "    fig_trigram.figure.savefig('./'+name+'_trigram.png')\n",
        "\n",
        "    return fig_bigram, fig_trigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5lKQX18waJy"
      },
      "source": [
        "dw_bigram, dw_trigram = grams_plot(dw_df,'DW')\n",
        "zeit_bigram, zeit_trigram = grams_plot(zeit_df,'ZEIT')\n",
        "faz_bigram, faz_trigram = grams_plot(faz_df,'FAZ')\n",
        "fr_bigram, fr_trigram = grams_plot(fr_df,'FR')\n",
        "spiegel_bigram, spiegel_trigram = grams_plot(spiegel_df,'SPIEGEL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41X-c85wlGn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}