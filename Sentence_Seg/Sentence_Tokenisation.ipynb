{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic method of Sentence Tokenization \n",
    "The is a very basic method of tokenizing a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Mr. James told me Dr. Brown is not available today. I will try tomorrow.\n",
      "-->Sentence 0: Mr\n",
      "-->Sentence 1: .\n",
      "-->Sentence 2:  James told me Dr\n",
      "-->Sentence 3: .\n",
      "-->Sentence 4:  Brown is not available today\n",
      "-->Sentence 5: .\n",
      "-->Sentence 6:  I will try tomorrow\n",
      "-->Sentence 7: .\n",
      "-->Sentence 8: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\"\n",
    "\n",
    "print('Original Article: %s' % (sentence))\n",
    "    \n",
    "sentences = re.split('(\\.|!|\\?)', sentence)\n",
    "    \n",
    "for i, s in enumerate(sentences):\n",
    "    print('-->Sentence %d: %s' % (i, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this better we can create a model and train them to understand the abbreviations and other non-sentence endings. But this has already been done by language models such as NLTK and Spacy. So, it is easier to use them. \n",
    "\n",
    "To install spacy use the following code:\n",
    "\n",
    "pip install -U spacy\n",
    "\n",
    "Loading the model \n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 : Mr. James told me Dr. Brown is not available today.\n",
      "Sentence 1 : I will try tomorrow.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print('Sentence %d : %s' %(i,sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['this is a sentence...', 'hello...and another sentence.']\n",
      "After: ['this is a sentence...', 'hello...', 'and another sentence.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = u\"this is a sentence...hello...and another sentence.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Before:\", [sent.text for sent in doc.sents])\n",
    "\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
    "doc = nlp(text)\n",
    "print(\"After:\", [sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 : Mr. James told me Dr. Brown is not available today. \n",
      "Sentence 1 : I will try tomorrow. \n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "doc = sent_tokenize(sentence)\n",
    "for i, sent in enumerate(doc):\n",
    "    print('Sentence %d : %s ' %(i,sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
