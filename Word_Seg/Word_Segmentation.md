Word Tokenization 

Basics of Tokenization 

https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en

NLTK 

https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html

https://nlp.stanford.edu/IR-book/html/htmledition/normalization-equivalence-classing-of-terms-1.html#sec:normalization

Source code

https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py

Explanation of  the Tokenizers

https://necromuralist.github.io/text-processing/

Spacy 

https://spacy.io/usage/linguistic-features#tokenization

Blog

https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/

https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3

There is also something for detokenizing

https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence

Custom tokenization of words can be done using Regexâ€”> regular Expressions

Text cleaning - basic methods 

https://machinelearningmastery.com/clean-text-machine-learning-python/

Segmentation 

https://hpi.de/fileadmin/user_upload/fachgebiete/plattner/teaching/NaturalLanguageProcessing/NLP2017/NLP03_TokenizationSegmentation.pdf

A nice blog on Segmentation of  noisy compounded words

https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da

